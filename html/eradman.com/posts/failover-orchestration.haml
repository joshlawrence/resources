<h2>Failover Orchestration for PostgreSQL</h2>

%h2 Ground Rules

%p
  The strong-consistory guarantees of a relational database does not generally
  lead to an architecture conducive to easy migration. Elaborate mechanisms can
  be conceived of to handle hot failover with PostgreSQL, but if there is one
  lesson that we can take away from Computer Science it is that the key to
  solving complex problems is to make assertions.

%p
  For this exercise we will use the following rules:

%ol
  %li
    Read-write connections are handled by a proxy (port 6432). By insisting that
    update operations run through a proxy we can instantly redirect transactions
    to a new backend while we wait for DNS to converge. The proxy also enables
    us to pause connections for the 5-8 seconds it will take to transition to a
    new master.
  %li
    Read-only connections are handled directly by postgres (port 5432).
    Applications that only need to read data will continue to operate on a
    standby that was promoted as a master. Again until DNS or some other
    mechanism shifts this load back to the new standby.
  %li
    Only one master is enabled at any given point in time. If we never have
    concurrent writes timelines will not diverge, eliminating the need to run a
    rewind.

%h2 The Salt Framework

%p
  For our basic salt configuration we will add two entries to
  <em>/etc/salt/master</em>:

%p
:codeblock
  :::sls
  file_roots:
    base:
      - /home/eradman/salt

  pillar_roots:
    base:
      - /home/eradman/salt/pillar

%p
  Where <em>file_roots</em> defines where to look for our state files, and
  <em>pillar_roots</em> defines where state files that define global variables
  are located.

:codeblock
  :::sls
  # top.sls
  {{saltenv}}:
    'pgdb*':
      - postgres

:codeblock
  :::sls
  # pillar/top.sls
  {{saltenv}}:
    '*':
      pg:
        mydb:
          writer: pgdb1
          query: pgdb2

      pg_replication_map:
        pgdb2: pgdb1

%p
  Now we will give salt some directions for managing our PostgreSQL instances
  using the static pillar data we defined.

:codeblock
  :::sls
  # postgres/init.sls
  {% if grains.host in [pillar.pg.mydb.query] %}
  /pg_data/9.5/recovery.conf:
    file.managed:
      - user: postgres
      - group: postgres
      - source: salt://postgres/recovery.conf.j2
      - user: postgres
      - group: postgres
      - template: jinja
      - context:
        restore_cmd: 'scp {{pillar.pg_replication_map[grains.host]}}:/pg_data/{{pgver}}/pg_xlog/%f

  {% else %}
  promote_new_master:
    cmd.run:
      - name: '/usr/pgsql-9.5/bin/pg_ctl promote -D /pg_data/9.5'
      - onlyif: 'test -e /pg_data/9.5/recovery.conf'
      - runas: postgres
  {% endif %}

  postgresql-9.5:
    service.running:
      - enable: True

%p
  Each standby will install <em>recovery.conf</em>, which will be rendered with
  context from <em>postgres/init.sls</em>

:codeblock
  :::sls
    # postgres/recovery.conf.j2
    standby_mode = 'on'
    primary_conninfo = 'user=postgres host={{ pillar.pg_replication_map[grains.host] }}'
    recovery_target_timeline = 'latest'
    restore_command = '{{restore_cmd}}'

%p
  Here a database is either a standby (query) or it is a master (writer). If
  it is a standby we install </em>recovery.conf</em>.
  <em>recovery_target_timeline</em> is an important parameter because it
  allows a standby to automatically follow a new timeline.

%p
  If the host is  not a standby but <em>recovery.conf</em> is found on the
  file system then we know a transition is taking place and we promote the
  server. In any case we ensure that the service is running. We can test to
  see what actions will be taken on each host individually:

:codeblock
  salt 'pgdb1*' state.highstate saltenv=eradman test=True
  salt 'pgdb2*' state.highstate saltenv=eradman test=True

%h2 Redirection

%p
  PgBouncer has the very nice property of pausing connections if the backend
  (the postgresql server) is not responding). By shutting down the former
  master we also avoid the danger of stray WAL statements during this
  transition.

%p
  To reconfigure pgbouncer we re-write <em>pgbouncer.ini</em> and signal a
  configuration change

:codeblock
  :::sls
  # postgres/init.sls
  /etc/pgbouncer/pgbouncer.ini:
    file.managed:
      - source: salt://postgres/pgbouncer.ini.j2
      - template: jinja

  kill -HUP `cat /var/run/pgbouncer/pgbouncer.pid`:
    cmd.run:
      - runas: pgbouncer
      - onchanges:
        - file: /etc/pgbouncer/pgbouncer.ini

%p
  Now in <em>pgbouncer.ini</em> we add some conditional configuration to point
  to a local UNIX socket or to a remote host. In this way connections to the
  proxy are always routed to a server that has write access.

:codeblock
  :::sls
  [databases]
  ; local unix connections
  {% if grains.host in [pillar.pg.mydb.writer] -%}
  database1 =
  database2 =
  {% endif %}

  ; redirections to current master
  {%- if grains.host == pillar.pg.mydb.query %}
  * = host={{ pillar.pg.mydb.writer }}
  {%- endif %}



%h2 Orchestration

%p
  Once we have the building blocks in place for connecting a stanby to the
  current master and promoting a stanby to a master we need to execute these in
  the right order. A schell script could do or we can use Salt's
  <em>state.orchestrate</em> module, which provides both sequential execution
  and the ability to specify dependencies.

:codeblock
  :::sls
  stop_former_master:
    salt.function:
      - name: cmd.run
      - tgt: {{pillar.pg.mydb.query}}*
      - arg:
        - systemctl stop postgresql-9.5

  promote_master:
    salt.state:
      - tgt: {{pillar.pg.mydb.writer}}*
      - highstate: True

  rejigger_replicas:
    salt.state:
      - tgt: {{pillar.pg.mydb.query}}*
      - highstate: True

%p
  That is the core of it, I also like to finish with a command that gives a
  one-line status of the new master's WAL status:

:codeblock
  :::sls
  mydb.writer_status:
    salt.function:
      - name: cmd.run
      - tgt: {{pillar.pg.mydb.writer}}*
      - arg:
        - |
          psql -U postgres <<SQL
             SELECT pg_xlogfile_name(pg_last_xlog_replay_location());
          SQL
<br />

%p
  That everything! Initiate failover using

:codeblock
  $EDITOR pillar/globals.sls
  sudo salt-run state.orchestrate postgres-failover.mydb saltenv=$USER

%h2 Log Messages

%p Transition to master:

:codeblock
  LOG:  received promote request
  LOG:  redo done at 32/39225840
  LOG:  last completed transaction was at log time 2016-12-01 10:30:52.907917-05
  LOG:  selected new timeline ID: 2
  LOG:  archive recovery complete
  LOG:  MultiXact member wraparound protections are now enabled
  LOG:  checkpoint starting: force
  LOG:  autovacuum launcher started

%p Transition to standby:

:codeblock
  LOG:  entering standby mode
  LOG:  consistent recovery state reached at 32/392258B0
  LOG:  invalid record length at 32/392258B0
  LOG:  database system is ready to accept read only connections
  LOG:  fetching timeline history file for timeline 2 from primary server
  LOG:  started streaming WAL from primary at 32/39000000 on timeline 1
  DETAIL:  End of WAL reached on timeline 1 at 32/392258B0.
  LOG:  new target timeline is 2
  LOG:  replication terminated by primary server
  LOG:  restarted WAL streaming at 32/39000000 on timeline 2
  LOG:  redo starts at 32/392258B0

%h2 A Replication View

%p
  Another aspect of replication and failover the is not obvious is how to
  collect meaningful information about the status of database replicas. Making
  this inquiry efficient is more important as the number of databases servers
  grows.

%p
  Again Salt helps because we can collect the list of involved hosts to run
  against

:codeblock
  :::sls
  {% set hosts = pillar.pg.mydb.values()|join("|") %}

%p
  Now we can find out the basic status of replication connections

:codeblock
  :::sls
  mydb_replication_status:
    salt.function:
      - name: cmd.run
      - tgt: '{{hosts}}'
      - tgt_type: pcre
      - arg:
        - |
          echo
          psql -U postgres <<SQL
            SELECT
              application_name, client_addr, state, sent_location, replay_location, sync_state
            FROM pg_stat_replication;
          SQL
<br />
%p
  Execute by running the SLS

:codeblock
  $ sudo salt-run state.sls mydb-status

%p
  A more in-depth view of reply status can be obtained from the
  <em>pg_control_checkpoint</em> view, which was introduced in 9.6

:codeblock
  :::sls
  qmdb_replay_status:
    salt.function:
      - name: cmd.run
      - tgt: '{{hosts}}'
      - tgt_type: pcre
      - arg:
        - |
          echo
          psql -U postgres <<SQL
            SELECT
              pg_last_xlog_receive_location() receive_location,
              pg_last_xlog_replay_location() replay_location,
              redo_wal_file, timeline_id, next_xid,
              date_trunc('second', now()-checkpoint_time) AS last_checkpoint,
              CASE WHEN pg_is_in_recovery()='t'
                THEN date_trunc('seconds', now()-pg_last_xact_replay_timestamp())
              END AS last_xact_replay
            FROM pg_control_checkpoint();
          SQL
<br />
%p
  For standard standby servers this will enable you to verify that all of the
  database servers are running on the same timeline, that they have not fallen
  behind, and for delayed replicas you can verify that they are running behind
  with the prescribed delay. In this example mydb3 has
  <em>recovery_min_apply_delay = '1d'</em> set in <em>recovery.conf</em>

:codeblock
        ID: mydb_replay_status
  Function: salt.function
      Name: cmd.run
    Result: True
   Comment: Function ran successfully. Function cmd.run ran on mydb1, mydb2, mydb3.
   Started: 16:12:50.445661
  Duration: 5072.736 ms
   Changes:

            mydb1:
             receive_location | replay_location |      redo_wal_file       | timeline_id |  next_xid  | last_checkpoint | last_xact_replay
            ------------------+-----------------+--------------------------+-------------+------------+-----------------+------------------
             EB/6FF5B540      | EB/6FF5B540     | 00000003000000EB0000006F |           3 | 0:33686826 | 00:08:21        | 00:00:12
            (1 row)

            mydb2:
             receive_location | replay_location |      redo_wal_file       | timeline_id |  next_xid  | last_checkpoint | last_xact_replay
            ------------------+-----------------+--------------------------+-------------+------------+-----------------+------------------
                              |                 | 00000003000000EB0000006F |           3 | 0:33686826 | 00:03:21        |
            (1 row)

            mydb3:
             receive_location | replay_location |      redo_wal_file       | timeline_id |  next_xid  | last_checkpoint | last_xact_replay
            ------------------+-----------------+--------------------------+-------------+------------+-----------------+------------------
             EB/6FF5B540      | EA/C87286A0     | 00000003000000EA000000C7 |           3 | 0:33656828 | 1 day 00:09:50  | 1 day 00:00:02
            (1 row)

