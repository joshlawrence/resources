%h1 Streaming Backups

%p
  Sometimes you take up a task because it needs to be done even though no one
  will notice. Setting up backups with real-time updates and point-in-time
  recovery may go unnoticed for a period, but it is not a thankless task.
%p
  The first time something very bad happens you will have a good story to tell.
  This story includes the fact that we have proper backups, but it also includes
  the changes and updates that you make every day to enable applications to
  evolve and adapt. Daily changes are best made under conditions are calm.
%p
  Mistakes will happen, and when they do a full database replica with streaming
  updates provide the means of recovering. Not living in terror of data loss
  means you can get a good nights rest, and write better software during the
  day. If this software can be moved into production without too much fuss you
  will definitely be thanked.

%h2 Making a Copy

%p
  In times past rsync (bracketed with invocations to
  <em>pg_start_backup()</em>, <em>pg_stop_backup()</em>) was the most
  effective way of brining up a new standby server, but these days <a
  href="http://www.postgresql.org/docs/9.5/static/app-pgbasebackup.html">pg_basebackup</a>
  is usually safer and more elegant. It operates by connecting as a
  client, so we need configure the database server as a master first by
  amending <em>postgresql.conf</em>

:codeblock
  :::config
  # postgresql.conf
  wal_level = hot_standby
  max_wal_senders = 4
  wal_keep_segments = 100

%p
  The value of <em>wal_keep_segments</em> instructs the server to keep at least
  this number of WAL logs. This will allow a replica to catch up if it is
  disconnected. For the purpose of a backup it also allows us to sync up after
  the initial clone.

%p
  After a reload we can create a copy of the master using

:codeblock
  :::sh
  pg_basebackup -U postgres -D backup

%p
  Hopefully you are taking this one step further to create an off-site replica.
  We can use <a href="http://www.ivarch.com/programs/pv.shtml">pv</a> utility or
  the built-in <em>--max-rate</em> option to rate-limit the stream over a WAN:

:codeblock
  :::sh
  mkdir -m 0700 backup
  cd backup
  pg_basebackup -h primary -U postgres -D - -Ft -x -z -vP \
         --max-rate=10M | tar -xzf -
%p
  The options supplied to <em>pg_basebackup</em> will stream the results over
  STDOUT in the tar format while displaying periodic progress updates on STDERR.

:codeblock
  LOG:  transaction log start point: 0/5000024 on timeline 1
  LOG:  42286/42286 kB (100%), 1/1 tablespace
  LOG:  transaction log end point: 0/50000DC
  LOG:  pg_basebackup: base backup completed

%p
  Note that everything is copied from the master, including configuration files
  and log files. Before cloning a busy server be sure to trim the contents of
  <em>/pg_log</em>

%p
  Starting a server to access the new backup is now as easy as starting up the
  server using the cloned data directory

:codeblock
  :::sh
  cp -R backup backup_copy
  pg_ctl -D backup_copy start

%p
  Tailing <em>pg_log/*</em> sould indicate that the server was able to reach a
  consistent state

:codeblock
  LOG:  creating missing WAL directory "pg_xlog/archive_status"
  LOG:  redo starts at 36/750000C8
  LOG:  consistent recovery state reached at 36/75751FC8


%h2 Point-in-Time Recovery

%p
  Now that we have a clone, <a
  href="http://www.postgresql.org/docs/9.5/static/app-pgreceivexlog.html">pg_receivexlog</a>
  can be used to stream transaction logs from the primary

:codeblock
  :::sh
  pg_receivexlog -h primary -U postgres -D backup

%p
  Running a replica will usually protect you from a hardware failure, it will
  not protect you from operator error or a faulty application.
  <a
  href="http://www.postgresql.org/docs/9.5/static/continuous-archiving.html">Point-in-time
  recovery</a> enables you to replay the transactions up to a specific point in
  time. It is activated by defining <em>recovery_target_time</em> in
  <em>recovery.conf</em> then Postgres will replay each transaction log until
  the specified timestamp

:codeblock
  :::config
  # recovery.conf
  recovery_target_time = '2015-06-29 08:00:00'
  restore_command = 'cp /pg_backup/wals/%f %p'

%p
  The <em>restore_command</em> is used to copy files from an transaction log
  archive.  The examples above are not using the archive feature, but this
  command is still required. When the server is started the
  <em>restore_command</em> is executed, will fail (because
  <em>/pg_backup/pg_xlog/</em> does not exist) and the data used in
  <em>pg_xlog</em> will be used instead.

:codeblock
  :::sh
  pg_ctl -D backup start

%p
  Postgres will record that it is running a point-in-time recovery to the
  specified time

:codeblock
  LOG:  starting point-in-time recovery to 2015-06-29 08:00:00-04
  LOG:  database system was not properly shut down; automatic recovery in
  LOG:  redo starts at 36/756E53B8

%p
  Once the restore succeeds <em>recovery.conf</em> is renamed to
  <em>restore.conf</em> and the server starts up in master mode with a new
  timeline.

%p
  All of this can be scripted and orchestrated, in my view this is exactly what
  <a href="http://www.pgbarman.org/">Barman</a> does, and it does it well.

%h2 Establishing a Delayed Replica

%p
  Storing WAL using <em>pg_receivexlog</em> works well for keeping transaction
  logs, but there is no way to apply these regularly, so a fresh full backup
  needs to taken periodically. Starting a server in-place on the backup will
  replay the WALL up to date, but a new timeline will be established during this
  process which will prevent us from receiving further WAL updates.

%p
  [I think] the most elegant solution to this is to run "live backups" using a
  delayed replica.

:codeblock
  :::config
  # recovery.conf
  recovery_min_apply_delay = '5d'

%p
  Since these servers are mostly idling inbetween update appliation a bunch of
  them can be run on a single host using different port numbers, or my
  preference is to stand them up in limited footprint VMs in way nearly
  identical to every other standby.

%h2 Daily Verification

%p
  One way to verify a physical replica is to measure the delta between
  transaction IDs on the master and the backup

:codeblock
  :::sh
  backup_txid=$(ssh $host "psql -At -c 'SELECT txid_current()'")
  # copy database and start in place
  restore_txid=$(psql -At -c 'SELECT txid_current()')
  echo $restore_txid $backup_txid | awk -f compare.awk > $host.verify/compare.out

%p
  Admittedly relying an arbitrary TXID delta (I use 6000) can raise false
  positives, but it will not miss a replica who has fallen behind.

%p
:codeblock
  :::awk
  BEGIN { xid_max_delta=6000; }
  {
  if ($2 - $1 < xid_max_delta)
      print "xids",$0,"=> Pass";
  else
      print "xids",$0,"=> Fail";
  }

%p
  Finally, notification and alerting can be handled by some combination of
  syslog and e-mail

:codeblock
  :::sh
  logger -t "pg_verifybackup $host" -p local0.notice &lt; $host.verify/compare.out
  grep -q "Pass" $host $host.verify/compare.out

%p
  The <a href="https://bitbucket.org/eradman/pg-verify">pg-verify</a> project
  provides a simple implementation of this process.
